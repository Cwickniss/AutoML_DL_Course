{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95AQO0vhZnXk",
        "outputId": "a96902b9-6ddb-432a-e24f-fc2b2248d89d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autoPyTorch\n",
            "  Using cached autoPyTorch-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autoPyTorch) (2.1.4)\n",
            "Requirement already satisfied: torch>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from autoPyTorch) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from autoPyTorch) (0.18.1+cu121)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from autoPyTorch) (2.17.0)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0 (from autoPyTorch)\n",
            "  Using cached scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "!pip install autoPyTorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoPyTorch[forecasting]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3JtNH5kbOS1",
        "outputId": "a07573ab-1b73-4d8e-8ab3-e7dc5887d9b9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autoPyTorch[forecasting]\n",
            "  Using cached autoPyTorch-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autoPyTorch[forecasting]) (2.1.4)\n",
            "Requirement already satisfied: torch>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from autoPyTorch[forecasting]) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from autoPyTorch[forecasting]) (0.18.1+cu121)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from autoPyTorch[forecasting]) (2.17.0)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0 (from autoPyTorch[forecasting])\n",
            "  Using cached scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autoPyTorch.api.tabular_classification import TabularClassificationTask\n",
        "\n",
        "# data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "# initialise Auto-PyTorch api\n",
        "api = TabularClassificationTask()\n",
        "\n",
        "# Search for an ensemble of machine learning algorithms\n",
        "api.search(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    optimize_metric='accuracy',\n",
        "    total_walltime_limit=300,\n",
        "    func_eval_time_limit_secs=50\n",
        ")\n",
        "\n",
        "# Calculate test accuracy\n",
        "y_pred = api.predict(X_test)\n",
        "score = api.score(y_pred, y_test)\n",
        "print(\"Accuracy score\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "xZk2zUBVajwK",
        "outputId": "b777804e-e942-44f1-9359-0f341be38aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'autoPyTorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6f10a8c888ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabularClassificationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data and metric imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoPyTorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from autoPyTorch.api.time_series_forecasting import TimeSeriesForecastingTask\n",
        "\n",
        "# data and metric imports\n",
        "from sktime.datasets import load_longley\n",
        "targets, features = load_longley()\n",
        "\n",
        "# define the forecasting horizon\n",
        "forecasting_horizon = 3\n",
        "\n",
        "# Dataset optimized by APT-TS can be a list of np.ndarray/ pd.DataFrame where each series represents an element in the\n",
        "# list, or a single pd.DataFrame that records the series\n",
        "# index information: to which series the timestep belongs? This id can be stored as the DataFrame's index or a separate\n",
        "# column\n",
        "# Within each series, we take the last forecasting_horizon as test targets. The items before that as training targets\n",
        "# Normally the value to be forecasted should follow the training sets\n",
        "y_train = [targets[: -forecasting_horizon]]\n",
        "y_test = [targets[-forecasting_horizon:]]\n",
        "\n",
        "# same for features. For uni-variant models, X_train, X_test can be omitted and set as None\n",
        "X_train = [features[: -forecasting_horizon]]\n",
        "# Here x_test indicates the 'known future features': they are the features known previously, features that are unknown\n",
        "# could be replaced with NAN or zeros (which will not be used by our networks). If no feature is known beforehand,\n",
        "# we could also omit X_test\n",
        "known_future_features = list(features.columns)\n",
        "X_test = [features[-forecasting_horizon:]]\n",
        "\n",
        "start_times = [targets.index.to_timestamp()[0]]\n",
        "freq = '1Y'\n",
        "\n",
        "# initialise Auto-PyTorch api\n",
        "api = TimeSeriesForecastingTask()\n",
        "\n",
        "# Search for an ensemble of machine learning algorithms\n",
        "api.search(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    optimize_metric='mean_MAPE_forecasting',\n",
        "    n_prediction_steps=forecasting_horizon,\n",
        "    memory_limit=16 * 1024,  # Currently, forecasting models use much more memories\n",
        "    freq=freq,\n",
        "    start_times=start_times,\n",
        "    func_eval_time_limit_secs=50,\n",
        "    total_walltime_limit=60,\n",
        "    min_num_test_instances=1000,  # proxy validation sets. This only works for the tasks with more than 1000 series\n",
        "    known_future_features=known_future_features,\n",
        ")\n",
        "\n",
        "# our dataset could directly generate sequences for new datasets\n",
        "test_sets = api.dataset.generate_test_seqs()\n",
        "\n",
        "# Calculate test accuracy\n",
        "y_pred = api.predict(test_sets)\n",
        "score = api.score(y_pred, y_test)\n",
        "print(\"Forecasting score\", score)"
      ],
      "metadata": {
        "id": "I3HYcLhkak9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "import torchvision.datasets\n",
        "\n",
        "from autoPyTorch.pipeline.image_classification import ImageClassificationPipeline\n",
        "\n",
        "# Get the training data for tabular classification\n",
        "trainset = torchvision.datasets.FashionMNIST(root='../datasets/', train=True, download=True)\n",
        "data = trainset.data.numpy()\n",
        "data = np.expand_dims(data, axis=3)\n",
        "# Create a proof of concept pipeline!\n",
        "dataset_properties = dict()\n",
        "pipeline = ImageClassificationPipeline(dataset_properties=dataset_properties)\n",
        "\n",
        "# Train and test split\n",
        "train_indices, val_indices = sklearn.model_selection.train_test_split(\n",
        "    list(range(data.shape[0])),\n",
        "    random_state=1,\n",
        "    test_size=0.25,\n",
        ")\n",
        "\n",
        "# Configuration space\n",
        "pipeline_cs = pipeline.get_hyperparameter_search_space()\n",
        "print(\"Pipeline CS:\\n\", '_' * 40, f\"\\n{pipeline_cs}\")\n",
        "config = pipeline_cs.sample_configuration()\n",
        "print(\"Pipeline Random Config:\\n\", '_' * 40, f\"\\n{config}\")\n",
        "pipeline.set_hyperparameters(config)\n",
        "\n",
        "# Fit the pipeline\n",
        "print(\"Fitting the pipeline...\")\n",
        "\n",
        "pipeline.fit(X=dict(X_train=data,\n",
        "                    is_small_preprocess=True,\n",
        "                    dataset_properties=dict(mean=np.array([np.mean(data[:, :, :, i]) for i in range(1)]),\n",
        "                                            std=np.array([np.std(data[:, :, :, i]) for i in range(1)]),\n",
        "                                            num_classes=10,\n",
        "                                            num_features=data.shape[1] * data.shape[2],\n",
        "                                            image_height=data.shape[1],\n",
        "                                            image_width=data.shape[2],\n",
        "                                            is_small_preprocess=True),\n",
        "                    train_indices=train_indices,\n",
        "                    val_indices=val_indices,\n",
        "                    )\n",
        "             )\n",
        "\n",
        "# Showcase some components of the pipeline\n",
        "print(pipeline)"
      ],
      "metadata": {
        "id": "XP4HR3jLa0Lf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}